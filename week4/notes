ğŸŒ± Day 15 â€“ My DevOps Journey
ğŸ“Œ Topic: Terraform Modules â€“ Reusable Infrastructure
ğŸ§  What I Learned Today
Today I understood how to write modular Terraform code, making infrastructure reusable, cleaner, and more organized. Modules help avoid repetition and are perfect when working with large or multi-environment setups.
ğŸ” Key Concepts
âœ… Terraform Modules
â€¢	A module is simply a folder with .tf files.
â€¢	Helps reuse code like EC2/VPC setups multiple times.
â€¢	Keeps root configuration clean and manageable.
ğŸ“ My Module Structure
modules/
â””â”€â”€ ec2/
    â”œâ”€â”€ main.tf
    â”œâ”€â”€ variables.tf
    â””â”€â”€ outputs.tf
ğŸ”— Using a Module in Main Configuration
module "my_ec2_instance" {
  source        = "./modules/ec2"
  instance_type = "t2.micro"
  ami           = "ami-xxxxxxxxxxxxxx"
}
ğŸ§° Input & Output
Inputs (variables.tf)
variable "instance_type" {
  type    = string
  default = "t2.micro"
}
Outputs (outputs.tf)
output "instance_id" {
  value = aws_instance.my_ec2.id
}
ğŸ§ª What Iâ€™ll Practice Next
â€¢	Break my old EC2 + S3 setups into modules.
â€¢	Call the same module with different inputs.
â€¢	Create separate modules for VPC, security group, and RDS.
âœ¨ My Takeaway
â€œModules are like Terraformâ€™s building blocks â€” reusable, neat, and ready for scaling!â€

ğŸŒ± Day 16 â€“ My DevOps Journey
ğŸ“Œ Topic: Dynamic Terraform with Data Sources
ğŸ§  What I Learned Today
I explored how to make Terraform more flexible and responsive by using data sources and locals. I learned how to pull real-time values like AMI IDs, distribute EC2 instances across availability zones, and build security rules dynamically.
ğŸ” Key Concepts
âœ… Data Sources
â€¢	Used data blocks like aws_ami and aws_availability_zones
â€¢	Automatically fetched the most recent AMI ID for Amazon Linux
âœ… Locals & Conditions
â€¢	Defined a default region using locals
â€¢	Used logical operations for cleaner configs
âœ… Multiple EC2 Instances
â€¢	Used count and element() to deploy EC2s across AZs
âœ… Dynamic Security Group
â€¢	Created a list of ingress rules and used a dynamic block
ğŸ“ Sample Code
data "aws_ami" "amazon_linux" {
  most_recent = true
  owners      = ["amazon"]
  filter {
    name   = "name"
    values = ["amzn2-ami-hvm-*-x86_64-gp2"]
  }
}

locals {
  ingress_rules = [
    { port = 22, description = "SSH" },
    { port = 80, description = "HTTP" },
    { port = 443, description = "HTTPS" },
    { port = 8080, description = "Custom" }
  ]
}

resource "aws_security_group" "dynamic_sg" {
  name        = "dynamic-sg"
  vpc_id      = var.vpc_id

  dynamic "ingress" {
    for_each = local.ingress_rules
    content {
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
ğŸ§ª What Iâ€™ll Practice Next
â€¢	Fetch dynamic subnet IDs and use them
â€¢	Write a custom locals.tf file for cleaner configs
â€¢	Add egress rules using dynamic blocks
âœ¨ My Takeaway
â€œTerraform becomes smarter when it adapts to your cloud environment automatically.â€


ğŸŒ± Day 17 â€“ My DevOps Journey
ğŸ“Œ Topic: User Data & Lifecycle Meta-Arguments
ğŸ§  What I Learned Today
I learned how to automatically configure EC2 instances on launch using user data scripts. I also learned how to manage resource updates safely with lifecycle meta-arguments.
ğŸ” Key Concepts
âœ… User Data
â€¢	Shell script installs Git, Maven, Apache
â€¢	Passed to EC2 instance using user_data = file(...)
âœ… Lifecycle Management
â€¢	Used create_before_destroy to avoid downtime
ğŸ“ install_httpd.sh Script
#!/bin/bash
sudo yum update -y
sudo yum install git -y
sudo yum install maven -y
sudo yum install -y httpd
sudo systemctl start httpd
sudo systemctl enable httpd
echo "<h1>Server launched via Terraform</h1>" > /var/www/html/index.html
ğŸ“ EC2 with Lifecycle
resource "aws_instance" "web" {
  ami           = data.aws_ami.amazon_linux.id
  instance_type = "t2.micro"
  key_name      = var.key_name
  user_data     = file("install_httpd.sh")

  lifecycle {
    create_before_destroy = true
  }
}
ğŸ§ª What Iâ€™ll Practice Next
â€¢	Use user_data to configure MySQL or Tomcat
â€¢	Combine with remote-exec for more control
âœ¨ My Takeaway
â€œUser data turns EC2s into ready-to-use servers right at boot time.â€



ğŸŒ± Day 18 â€“ My DevOps Journey
ğŸ“Œ Topic: Terraform Remote State + Locking
ğŸ§  What I Learned Today
Today I moved from local to remote backend using S3, and added state locking using DynamoDB â€” a key step toward real-world teamwork and collaboration.
ğŸ” Key Concepts
âœ… Remote State
â€¢	Stores .tfstate in S3 instead of local machine
â€¢	Helps share infra state with teams
âœ… DynamoDB Locking
â€¢	Prevents concurrent applies from corrupting state
â€¢	Uses a LockID primary key
ğŸ“ Backend Configuration
terraform {
  backend "s3" {
    bucket         = "awsdevops8amterraformstate"
    key            = "terraform.tfstate"
    region         = "ap-south-1"
    encrypt        = true
    dynamodb_table = "awsdevops8amdynamodb"
  }
}
ğŸ§ª What Iâ€™ll Practice Next
â€¢	Use different backends for dev/staging/prod
â€¢	Add versioning and lifecycle policy to the S3 bucket
âœ¨ My Takeaway
â€œRemote state + locking = safe, scalable Terraform in team environments.â€

ğŸŒ± Day 19 â€“ My DevOps Journey
ğŸ“Œ Topic: Reusable Infrastructure with Modules
ğŸ§  What I Learned Today
Today I understood how to write modular Terraform code, making infrastructure reusable, cleaner, and more organized. Modules help avoid repetition and are perfect when working with large or multi-environment setups.
ğŸ” Key Concepts
âœ… Terraform Modules
â€¢	A module is simply a folder with .tf files
â€¢	Helps reuse code like EC2/VPC setups multiple times
â€¢	Keeps root configuration clean and manageable
ğŸ“ My Module Structure
modules/
â””â”€â”€ ec2/
    â”œâ”€â”€ main.tf
    â”œâ”€â”€ variables.tf
    â””â”€â”€ outputs.tf
ğŸ“ Calling the Module
module "my_ec2_instance" {
  source        = "./modules/ec2"
  instance_type = "t2.micro"
  ami           = "ami-xxxxxxxxxxxxxx"
}
ğŸ“ Inputs
variable "instance_type" {
  type    = string
  default = "t2.micro"
}
ğŸ“ Outputs
output "instance_id" {
  value = aws_instance.my_ec2.id
}
ğŸ§ª What Iâ€™ll Practice Next
â€¢	Break my old EC2 + S3 setups into modules
â€¢	Call the same module with different inputs
â€¢	Create separate modules for VPC, security group, and RDS
âœ¨ My Takeaway
â€œModules are like Terraformâ€™s building blocks â€” reusable, neat, and ready for scaling!â€

ğŸŒ± Day 20 â€“ My DevOps Journey
ğŸ“Œ Topic: Git Refresher + Ansible Setup
ğŸ§  What I Learned Today
I started the day by revisiting important Git concepts and finished by setting up Ansible for the first time â€” getting ready to manage servers at scale.
ğŸ” Key Concepts
âœ… Git Basics
â€¢	git init, git clone, git add, git commit, git push
â€¢	Branching: git checkout -b, git branch -d
â€¢	Merge vs Rebase: tested both
â€¢	Cherry-pick: grabbed specific commits
âœ… Ansible Setup
â€¢	Generated SSH key with ssh-keygen
â€¢	Copied key to remote machines
â€¢	Installed Ansible and verified Python3
â€¢	Created /etc/ansible/hosts inventory file
ğŸ“ Inventory File Example
[webservers]
54.226.3.181
34.224.7.217
ğŸ§ª Commands Practiced
ansible -m ping all
ansible -i inventory.ini -m ping webservers
ğŸ§ª What Iâ€™ll Practice Next
â€¢	Create and run Ansible playbooks
â€¢	Manage multiple servers from one control node
âœ¨ My Takeaway
â€œWith Git and Ansible together, Iâ€™m ready to automate both code and infrastructure.â€


